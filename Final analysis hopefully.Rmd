---
title: "Association between sleep quality and pain in older adults"
author: "Pedro Henrique Brant"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

## Introduction

In this manuscript, we're going to conduct an analysis into the ELSI Dataset. This is a modified version for usage in the Principles and Practice of Clinical Research program of Harvard T.H. Chan School of Public Health.

The aim here is to replicate the analysis done in a [previous paper](https://pubmed.ncbi.nlm.nih.gov/34968438/) and see if the results also apply to the sample that was included in the [ELSI Brazil](https://elsi.cpqrr.fiocruz.br/en/home-english/).

First order of business is to download the dataset we're going to be analyzing, which was kindly provided by PPCR staff.

```{r, message=FALSE, warning=FALSE}
## Loading the necessary libraries
library(tidyverse)
library(haven)
library(broom)
library(ggplot2)
library(lmtest)
```

```{r}

## Url provided by PPCR staff
url <- 'https://contattafiles.s3.us-west-1.amazonaws.com/tnt45405/T9gKrWnaRr5K0yT/FINAL%20-%20ELSI_students.dta'

## Using the read_dta function from haven to read the .dta file created in STATA
PPCRdata <- read_dta(url)

## Removing the url from memory as it is no longer useful
rm(url)

## Transforming to tibble in order to increase readability
PPCRdata <- as_tibble(PPCRdata)
PPCRdata

```

PPCR Staff has described the variables included in this dataset with the following:
![Variables in modified dataset](https://contattafiles.s3.us-west-1.amazonaws.com/tnt45405/3Rq88vSRw93ITf6/Screen%20Shot%202023-10-04%20at%201.03.35%20PM.png)

## Adjusting Variables for the Analysis

Let's now look at the some of the individual variables to understand what they mean
```{r}
unique(PPCRdata$pain)
unique(PPCRdata$p_intensity)
unique(PPCRdata$sleepquality)
unique(PPCRdata$sleepproblems)
```

We can then see that *pain* and *sleepproblems* are binary variables, while *p_intensity* and *sleepquality* are ordinal variables, although they aren't coded in this manner yet.

Now let's transform *p_intensity* and *sleepquality* into ordinal variables and *pain* and *sleepproblems* into binary variables, as they were read as numerical variables with labels.

```{r}
## Using the as_factor function from haven to transform the variables 
## with the labels as names for the levels in the factors
PPCRdata$sleepquality <- as_factor(PPCRdata$sleepquality, ordered = TRUE)
PPCRdata$p_intensity <- as_factor(PPCRdata$p_intensity, ordered = TRUE)
PPCRdata$pain <- as_factor(PPCRdata$pain)
PPCRdata$sleepproblems <- as_factor(PPCRdata$sleepproblems)


levels(PPCRdata$sleepquality)
levels(PPCRdata$p_intensity)
levels(PPCRdata$pain)
levels(PPCRdata$sleepproblems)

## Reversing the order of factors, as initially the order was wrong in sleepquality
PPCRdata$sleepquality <- fct_rev(PPCRdata$sleepquality)
levels(PPCRdata$sleepquality)

```

Other variables that we're going to use in our model that haven't been coded properly as well include: *depression*, *sex*, *alcohol*, *fracture*, *cancer*, *diabetes*, *dementia*, *arthritis*, *heartdisease* and *lungdisease*.
```{r}
## Showing that the variables are coded as numerical variables
unique(PPCRdata$depression)
unique(PPCRdata$sex)
unique(PPCRdata$alcohol)
unique(PPCRdata$fracture)
unique(PPCRdata$cancer)
unique(PPCRdata$diabetes)
unique(PPCRdata$dementia)
unique(PPCRdata$arthritis)
unique(PPCRdata$heartdisease)
unique(PPCRdata$lungdisease)

## Using the as_factor function from haven to transform the variables 
## with the labels as names for the levels in the factors
PPCRdata$depression <- as_factor(PPCRdata$depression)
PPCRdata$sex <- as_factor(PPCRdata$sex)
PPCRdata$alcohol <- as_factor(PPCRdata$alcohol)
PPCRdata$fracture <- as_factor(PPCRdata$fracture)
PPCRdata$cancer <- as_factor(PPCRdata$cancer)
PPCRdata$diabetes <- as_factor(PPCRdata$diabetes)
PPCRdata$dementia <- as_factor(PPCRdata$dementia)
PPCRdata$arthritis <- as_factor(PPCRdata$arthritis)
PPCRdata$heartdisease <- as_factor(PPCRdata$heartdisease)
PPCRdata$lungdisease <- as_factor(PPCRdata$lungdisease)
```
For completeness, let's check all of the variables we're going to use together in a summary
```{r}
PPCRdata %>% dplyr::select(age,bmi,sex,pain,p_intensity,sleepquality,sleepproblems,depression,
                    alcohol,fracture,cancer,diabetes,dementia,arthritis,
                    heartdisease,lungdisease) %>% summary()
```
At this point we have coded all the variables correctly, so we can proceed with the analysis.

## Analysis
### Association between *pain* and *sleepquality*
#### Unadjusted Analysis
This analysis checks for the association between the *pain* variable and the *sleepquality* variable, without adjustments. As *pain* is a binary variable, we're going to run a logistic regression.

```{r}
## using the generalized linear model function 
## with the family = "binomial" to create a logistical
## regression
model0 <- glm (data = PPCRdata, formula = pain ~ sleepquality,
               family = "binomial")
summary(model0)
```

As the independent variable is coded as ordinal, the interpretation isn't completely straightforward. To make it simpler, let's start by calculating confidence intervals (CI) and then exponentiating them together with the coefficients to obtain the odds ratios and their respective CI's.

```{r message=FALSE, warning=FALSE}

## coef(model0) extracts the coefficients from the model
## confint(model0) calculates confidence intervals (95%)
## cbind then creates a table by joining the two by column
## finally, we exponentiate all the results using the base e
## in order to transform logit units into regular units

exp(cbind(OR = coef(model0),confint(model0)))

## another way, which is simpler and does the same, is to use
## the broom::tidy function

model0Tidy <- tidy(model0, conf.int = TRUE, exponentiate = TRUE)
model0Tidy %>% rename(Odds.Ratio = estimate) -> model0Tidy
model0Tidy

## let's plot these coefficients and their CI's to increase
## readability

model0Tidy %>% ggplot (aes(Odds.Ratio, term, xmin = conf.low,
                           xmax = conf.high, height = 0)) +
                ## inserting a point for the odds ratio
                geom_point() +
                ## drawing a line at the 1 to improve readability
                geom_vline(xintercept = 1, lty = 4) +
                ## drawing an horizontal line to represent the 
                ## confidence interval
                geom_errorbarh() +
                ## reordering the categorical variables in the
                ## y-axis to improve readability
                ## also removing the intercept, as it's not 
                ## relevant for interpretation
                scale_y_discrete(limits = c('sleepquality.L',
                                            'sleepquality.Q',
                                            'sleepquality.C',
                                            'sleepquality^4'))

```

The p-value of the term *sleepquality.L* being significant means that there is a statistically significant **linear** association between <u>increasing</u> *sleepquality* values and <u>decreasing</u> the probability of *pain* being present (due to the less than 1 Odds Ratio).

We can also see that our model finds a significant relationship between *sleepquality.C* and *pain*. This finding is a little bit harder to interpret, but according to [this stack overflow post](https://stackoverflow.com/questions/57297771/interpretation-of-l-q-c-4-for-logistic-regression), we can interpret the third order polynomial as [jerk](https://en.wikipedia.org/wiki/Jerk_(physics)).

---

A different approach would be to either consider the variable as continuous or categorical. As we don't have to test for any assumptions in order to consider it categorical, let's proceed with that first.

```{r warning=FALSE}
## changing the variable to categorical
PPCRdata$sleepquality <- factor(PPCRdata$sleepquality, ordered = FALSE)

## running the model
model1 <- glm (data = PPCRdata, formula = pain ~ sleepquality,
               family = "binomial")
summary(model1)

## Let's proceed with the same transformations we did 
## when we built the model with the variable considered
## as ordinal, in order to improve readability

model1Tidy <- tidy(model1, conf.int = TRUE, exponentiate = TRUE)
model1Tidy %>% rename(Odds.Ratio = estimate) -> model1Tidy
model1Tidy

## Let's also plot this one for increased readability

model1Tidy %>% ggplot (aes(Odds.Ratio, term, xmin = conf.low,
                           xmax = conf.high, height = 0)) +
                ## inserting a point for the odds ratio
                geom_point() +
                ## drawing a line at the 1 to improve readability
                geom_vline(xintercept = 1, lty = 4) +
                ## drawing an horizontal line to represent the 
                ## confidence interval
                geom_errorbarh() +
                ## changing the order to increase readability
                ## and removing the intercept again
                scale_y_discrete(limits = c('sleepqualityBad',
                                            'sleepqualityFair',
                                            'sleepqualityGood',
                                            'sleepqualityVery good'))
                

## reestablishing the variable as ordinal
PPCRdata$sleepquality <- factor(PPCRdata$sleepquality, ordered = TRUE)
```
This analysis allows us to determine that, when using a reference level of *sleepquality* that is **very bad**, the odds of experiencing *pain* when *sleepquality* increases are significantly lower (the odds ratio of the comparison of each level with very bad sleep quality can be seen in the table above).

---

In this next model, we're going to consider sleepquality as a continuous variable.

```{r}
## doing the transformation and saving on another variable to not lose the
## categorical information (names for each level)
PPCRdata %>% mutate(sleepquality = as.numeric(sleepquality)) -> dataNumeric

## checking to see if it worked properly
head(dataNumeric$sleepquality)
```
Now that the variable is transformed to numeric, let's run the model again.

```{r}

glm (data = dataNumeric, formula = pain ~ sleepquality, 
     family = "binomial") -> model2
model2 %>% tidy(confint = TRUE, exponentiate = TRUE) %>% 
        rename(Odds.Ratio = estimate) -> model2Tidy
summary(model2)
model2Tidy

```
In order to determine which model is best, we're going to be conducting two tests: the Likelihood Ratio Chi-Square Test and the BIC test (as was recommended [here](https://www3.nd.edu/~rwilliam/stats3/OrdinalIndependent.pdf) by Richard Williams of the University of Notre Dame).

```{r}
lrtest(model1,model2)
```

The alternative hypothesis for this test 

#### Adjusted Analysis
This analysis checks for the association between the *pain* variable and the *sleepquality* variable, adjusting the model for age, bmi, comorbidities and depression, as was done in the original article. As *pain* is a binary variable, we're going to run a logistic regression.

```{r}
model3 <- glm(data = PPCRdata, formula = pain ~ sleepquality + bmi + depression + 
                       sex + alcohol + fracture + cancer + diabetes + dementia +
                       arthritis + heartdisease + lungdisease,
                                                        family = "binomial")
summary(model3)
```

The interpretation of this model regarding our main independent variable, *sleepquality*, is hard to do, as the model is considering it as ordinal. Let's run it again with the model considering it as categorical.

```{r}
## changing the variable to categorical
PPCRdata$sleepquality <- factor(PPCRdata$sleepquality, ordered = FALSE)

## running the model
model4 <- glm (data = PPCRdata, formula = pain ~ sleepquality + depression + 
                       sex + alcohol + fracture + cancer + diabetes + dementia +
                       arthritis + heartdisease + lungdisease,
                                                        family = "binomial")
summary(model4)

## reestablishing the variable as ordinal
PPCRdata$sleepquality <- factor(PPCRdata$sleepquality, ordered = TRUE)
```

According to the [UCLA Office of Advanced Research Computing (OARC)](https://stats.oarc.ucla.edu/r/dae/logit-regression/), in order to test for an overall effect of the *sleepquality* variable, we can employ the wald.test function from the aod library.

```{r, warning = FALSE}
library(aod)
wald.test(b = coef(model4), Sigma = vcov(model4), Terms = 2:5)
```
We can also compute the odds-ratios for each variable.

```{r}
exp(cbind(OR = coef(model4), confint(model4)))
```

#### Testing the assumptions of the model

At this point, I haven't yet learned how to properly test for the assumptions of the logistic regression model. On a later opportunity, I'll try to remedy this situation. For now, I'll include some commands that were listed as useful in the [R you ready for R?](https://bookdown.org/wadetroberts/r-you-ready-for-r/multiple-logistic-regression.html#model-fit-statistics-for-multiple-logistic-regression) e-book. 

```{r warning= FALSE}
library(blorr)
blr_model_fit_stats(model4)
blr_test_hosmer_lemeshow(model4)
blr_roc_curve(blr_gains_table(model4))
car::vif(model4)
```

### Association between *sleepquality* and *pain*

In order to do this analysis I'll try to follow the instructions from the [UCLA OARC - Ordinal Logistic Regression | R Data Analysis Example](https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/).

As the original article by [MontelhÃ£o et al](https://pubmed.ncbi.nlm.nih.gov/34968438/) tested for bidirectional associations, let's try to replicate that as well. In this case, the dependent variable is ordinal, therefore we will run an ordinal logistical regression.

```{r warning = FALSE, message = FALSE}
## loading the library required to run the model
library(MASS)

## running the model
model5 <- polr (data = PPCRdata, formula = sleepquality ~ pain + depression + 
                       sex + alcohol + fracture + cancer + diabetes + dementia +
                       arthritis + heartdisease + lungdisease, Hess = TRUE)
summary(model5)
```

In R, this doesn't produce the p-values, so let's add those, as our sample size is large enough that they are reasonable approximations.

```{r}
## getting the coefficients
ctable <- coef(summary(model5))

## getting the respective p-values from the normal distribution
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2

## creating a table with the respective p-values added
ctable <- cbind(ctable, "p value" = p)
ctable

## creating confidence intervals
ci<-confint.default(model5)

## exponentiating the log-odds and the confidence intervals
## in order to create the odds-ratios and the respective CI's
exp(cbind(OR = coef(model5), ci))

```

#### Testing for the Proportional Odds Assumption

This is something to consider further on. At this point, I haven't yet figured out exactly how to do it, but I'll try to follow the instructions from the [Office of Advanced Research Computing](https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/) on another day.

